===============================================================
Starting job on host: xgpi3
Job ID: 158797
Partition: gpu
===============================================================
Activating Python virtual environment...
/var/lib/slurm/slurmd/job158797/slurm_script: line 16: miniLM-LoRA/lora_env/bin/activate: No such file or directory
/var/lib/slurm/slurmd/job158797/slurm_script: line 19: cd: miniLM-LoRA: No such file or directory
Current directory: /home/e/e0958171/miniLM-LoRA
Starting LoRA training...
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.
Model Initialized.
  0%|          | 0/100 [00:00<?, ?it/s]/home/e/e0958171/miniLM-LoRA/lora_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  1%|          | 1/100 [00:02<04:06,  2.49s/it]  2%|▏         | 2/100 [00:03<03:00,  1.85s/it]  3%|▎         | 3/100 [00:05<02:36,  1.62s/it]  4%|▍         | 4/100 [00:06<02:26,  1.53s/it]  5%|▌         | 5/100 [00:07<02:19,  1.47s/it]  6%|▌         | 6/100 [00:09<02:13,  1.43s/it]  7%|▋         | 7/100 [00:10<02:08,  1.38s/it]  8%|▊         | 8/100 [00:11<02:05,  1.36s/it]  9%|▉         | 9/100 [00:13<02:02,  1.35s/it] 10%|█         | 10/100 [00:14<01:59,  1.33s/it]                                                 10%|█         | 10/100 [00:14<01:59,  1.33s/it] 11%|█         | 11/100 [00:15<01:58,  1.33s/it] 12%|█▏        | 12/100 [00:17<01:57,  1.33s/it] 13%|█▎        | 13/100 [00:18<01:55,  1.33s/it] 14%|█▍        | 14/100 [00:19<01:53,  1.32s/it] 15%|█▌        | 15/100 [00:21<01:52,  1.32s/it] 16%|█▌        | 16/100 [00:22<01:50,  1.31s/it] 17%|█▋        | 17/100 [00:23<01:48,  1.31s/it] 18%|█▊        | 18/100 [00:25<01:47,  1.31s/it] 19%|█▉        | 19/100 [00:26<01:46,  1.31s/it] 20%|██        | 20/100 [00:27<01:44,  1.31s/it]                                                 20%|██        | 20/100 [00:27<01:44,  1.31s/it] 21%|██        | 21/100 [00:28<01:43,  1.31s/it] 22%|██▏       | 22/100 [00:30<01:42,  1.31s/it] 23%|██▎       | 23/100 [00:31<01:40,  1.31s/it] 24%|██▍       | 24/100 [00:32<01:39,  1.31s/it] 25%|██▌       | 25/100 [00:34<01:38,  1.31s/it] 26%|██▌       | 26/100 [00:35<01:36,  1.31s/it] 27%|██▋       | 27/100 [00:36<01:35,  1.31s/it] 28%|██▊       | 28/100 [00:38<01:33,  1.30s/it] 29%|██▉       | 29/100 [00:39<01:32,  1.30s/it] 30%|███       | 30/100 [00:40<01:30,  1.30s/it]                                                 30%|███       | 30/100 [00:40<01:30,  1.30s/it] 31%|███       | 31/100 [00:41<01:29,  1.29s/it] 32%|███▏      | 32/100 [00:43<01:28,  1.30s/it] 33%|███▎      | 33/100 [00:44<01:26,  1.30s/it] 34%|███▍      | 34/100 [00:45<01:25,  1.29s/it] 35%|███▌      | 35/100 [00:47<01:24,  1.30s/it] 36%|███▌      | 36/100 [00:48<01:22,  1.29s/it] 37%|███▋      | 37/100 [00:49<01:21,  1.29s/it] 38%|███▊      | 38/100 [00:51<01:19,  1.29s/it] 39%|███▉      | 39/100 [00:52<01:18,  1.29s/it] 40%|████      | 40/100 [00:53<01:17,  1.29s/it]                                                 40%|████      | 40/100 [00:53<01:17,  1.29s/it] 41%|████      | 41/100 [00:54<01:15,  1.28s/it] 42%|████▏     | 42/100 [00:56<01:14,  1.29s/it] 43%|████▎     | 43/100 [00:57<01:13,  1.28s/it] 44%|████▍     | 44/100 [00:58<01:11,  1.28s/it] 45%|████▌     | 45/100 [01:00<01:10,  1.29s/it] 46%|████▌     | 46/100 [01:01<01:09,  1.28s/it] 47%|████▋     | 47/100 [01:02<01:08,  1.29s/it] 48%|████▊     | 48/100 [01:03<01:07,  1.30s/it] 49%|████▉     | 49/100 [01:05<01:05,  1.29s/it] 50%|█████     | 50/100 [01:06<01:04,  1.29s/it]                                                 50%|█████     | 50/100 [01:06<01:04,  1.29s/it] 51%|█████     | 51/100 [01:07<01:03,  1.29s/it] 52%|█████▏    | 52/100 [01:09<01:02,  1.29s/it] 53%|█████▎    | 53/100 [01:10<01:00,  1.29s/it] 54%|█████▍    | 54/100 [01:11<00:59,  1.29s/it] 55%|█████▌    | 55/100 [01:12<00:57,  1.29s/it] 56%|█████▌    | 56/100 [01:14<00:56,  1.29s/it] 57%|█████▋    | 57/100 [01:15<00:55,  1.29s/it] 58%|█████▊    | 58/100 [01:16<00:54,  1.29s/it] 59%|█████▉    | 59/100 [01:18<00:52,  1.29s/it] 60%|██████    | 60/100 [01:19<00:51,  1.28s/it]                                                 60%|██████    | 60/100 [01:19<00:51,  1.28s/it] 61%|██████    | 61/100 [01:20<00:49,  1.28s/it] 62%|██████▏   | 62/100 [01:21<00:48,  1.28s/it] 63%|██████▎   | 63/100 [01:23<00:47,  1.28s/it] 64%|██████▍   | 64/100 [01:24<00:46,  1.28s/it] 65%|██████▌   | 65/100 [01:25<00:44,  1.28s/it] 66%|██████▌   | 66/100 [01:27<00:43,  1.28s/it] 67%|██████▋   | 67/100 [01:28<00:42,  1.27s/it] 68%|██████▊   | 68/100 [01:29<00:40,  1.27s/it] 69%|██████▉   | 69/100 [01:30<00:39,  1.28s/it] 70%|███████   | 70/100 [01:32<00:38,  1.28s/it]                                                 70%|███████   | 70/100 [01:32<00:38,  1.28s/it] 71%|███████   | 71/100 [01:33<00:37,  1.28s/it] 72%|███████▏  | 72/100 [01:34<00:35,  1.28s/it] 73%|███████▎  | 73/100 [01:36<00:34,  1.28s/it] 74%|███████▍  | 74/100 [01:37<00:33,  1.28s/it] 75%|███████▌  | 75/100 [01:38<00:32,  1.28s/it] 76%|███████▌  | 76/100 [01:39<00:30,  1.28s/it] 77%|███████▋  | 77/100 [01:41<00:29,  1.28s/it] 78%|███████▊  | 78/100 [01:42<00:28,  1.28s/it] 79%|███████▉  | 79/100 [01:43<00:26,  1.29s/it] 80%|████████  | 80/100 [01:45<00:25,  1.29s/it]                                                 80%|████████  | 80/100 [01:45<00:25,  1.29s/it] 81%|████████  | 81/100 [01:46<00:24,  1.29s/it] 82%|████████▏ | 82/100 [01:47<00:23,  1.28s/it] 83%|████████▎ | 83/100 [01:48<00:21,  1.28s/it] 84%|████████▍ | 84/100 [01:50<00:20,  1.28s/it] 85%|████████▌ | 85/100 [01:51<00:19,  1.28s/it] 86%|████████▌ | 86/100 [01:52<00:17,  1.27s/it] 87%|████████▋ | 87/100 [01:53<00:16,  1.27s/it] 88%|████████▊ | 88/100 [01:55<00:15,  1.27s/it] 89%|████████▉ | 89/100 [01:56<00:14,  1.28s/it] 90%|█████████ | 90/100 [01:57<00:12,  1.28s/it]                                                 90%|█████████ | 90/100 [01:57<00:12,  1.28s/it] 91%|█████████ | 91/100 [01:59<00:11,  1.28s/it] 92%|█████████▏| 92/100 [02:00<00:10,  1.27s/it] 93%|█████████▎| 93/100 [02:01<00:08,  1.27s/it] 94%|█████████▍| 94/100 [02:02<00:07,  1.27s/it] 95%|█████████▌| 95/100 [02:04<00:06,  1.28s/it] 96%|█████████▌| 96/100 [02:05<00:05,  1.28s/it] 97%|█████████▋| 97/100 [02:06<00:03,  1.27s/it] 98%|█████████▊| 98/100 [02:07<00:02,  1.27s/it] 99%|█████████▉| 99/100 [02:09<00:01,  1.27s/it]100%|██████████| 100/100 [02:10<00:00,  1.27s/it]                                                 100%|██████████| 100/100 [02:10<00:00,  1.27s/it]                                                 100%|██████████| 100/100 [02:11<00:00,  1.27s/it]100%|██████████| 100/100 [02:11<00:00,  1.32s/it]
{'loss': 2.8996, 'grad_norm': 0.8289132714271545, 'learning_rate': 0.000182, 'entropy': 2.7426927417516707, 'num_tokens': 23866.0, 'mean_token_accuracy': 0.4241801891475916, 'epoch': 0.02}
{'loss': 2.7455, 'grad_norm': 0.7787217497825623, 'learning_rate': 0.000162, 'entropy': 2.6348689585924148, 'num_tokens': 46975.0, 'mean_token_accuracy': 0.43444034829735756, 'epoch': 0.05}
{'loss': 2.6947, 'grad_norm': 0.7928897142410278, 'learning_rate': 0.000142, 'entropy': 2.6708380579948425, 'num_tokens': 69563.0, 'mean_token_accuracy': 0.4346182432025671, 'epoch': 0.07}
{'loss': 2.6211, 'grad_norm': 0.8211758136749268, 'learning_rate': 0.000122, 'entropy': 2.614258220791817, 'num_tokens': 91476.0, 'mean_token_accuracy': 0.4437716122716665, 'epoch': 0.09}
{'loss': 2.6788, 'grad_norm': 0.8689077496528625, 'learning_rate': 0.00010200000000000001, 'entropy': 2.619282513856888, 'num_tokens': 113624.0, 'mean_token_accuracy': 0.43837476149201393, 'epoch': 0.11}
{'loss': 2.5976, 'grad_norm': 0.8489084243774414, 'learning_rate': 8.2e-05, 'entropy': 2.6081791639328005, 'num_tokens': 134122.0, 'mean_token_accuracy': 0.4511027358472347, 'epoch': 0.14}
{'loss': 2.5511, 'grad_norm': 0.7586027979850769, 'learning_rate': 6.2e-05, 'entropy': 2.5222576379776003, 'num_tokens': 154270.0, 'mean_token_accuracy': 0.4544701162725687, 'epoch': 0.16}
{'loss': 2.6267, 'grad_norm': 0.8543017506599426, 'learning_rate': 4.2e-05, 'entropy': 2.5597324162721633, 'num_tokens': 176819.0, 'mean_token_accuracy': 0.4416915435343981, 'epoch': 0.18}
{'loss': 2.5163, 'grad_norm': 0.9233940839767456, 'learning_rate': 2.2000000000000003e-05, 'entropy': 2.5261963486671446, 'num_tokens': 201363.0, 'mean_token_accuracy': 0.4513269167393446, 'epoch': 0.21}
{'loss': 2.569, 'grad_norm': 0.8362094163894653, 'learning_rate': 2.0000000000000003e-06, 'entropy': 2.5472298502922057, 'num_tokens': 223243.0, 'mean_token_accuracy': 0.4516935721039772, 'epoch': 0.23}
{'train_runtime': 131.5587, 'train_samples_per_second': 6.081, 'train_steps_per_second': 0.76, 'train_loss': 2.6500381088256835, 'epoch': 0.23}
LoRA complete!
===============================================================
Job finished successfully!
===============================================================
